{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92e79d0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-12T13:08:54.577593Z",
     "iopub.status.busy": "2025-12-12T13:08:54.577282Z",
     "iopub.status.idle": "2025-12-12T13:08:56.014431Z",
     "shell.execute_reply": "2025-12-12T13:08:56.013515Z"
    },
    "papermill": {
     "duration": 1.442477,
     "end_time": "2025-12-12T13:08:56.015849",
     "exception": false,
     "start_time": "2025-12-12T13:08:54.573372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/spartan0-1/tensorrtllm/default/2/h1_agent_safety.py\n",
      "/kaggle/input/spartan0-1/tensorrtllm/default/2/ppf_colab_notebook_primes__physics.md\n",
      "/kaggle/input/spartan0-1/tensorrtllm/default/2/model.json\n",
      "/kaggle/input/spartan0-1/tensorrtllm/default/2/lambda_pipeline_fixed.py\n",
      "/kaggle/input/spartan0-1/tensorrtllm/default/2/harmonic_matrix_full.csv\n",
      "/kaggle/input/spartan0-1/tensorrtllm/default/2/lambda_fragility_benchmark_1024bit.json\n",
      "/kaggle/input/spartan0-1/tensorrtllm/default/2/h1_novel_predictions.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/AIMO3_Reference_Problems.pdf\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/sample_submission.csv\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/aimo_3_inference_server.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/aimo_3_gateway.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/__init__.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/templates.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/base_gateway.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/relay.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/kaggle_evaluation.proto\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/__init__.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/generated/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba456f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:08:56.022425Z",
     "iopub.status.busy": "2025-12-12T13:08:56.022117Z",
     "iopub.status.idle": "2025-12-12T13:08:58.340959Z",
     "shell.execute_reply": "2025-12-12T13:08:58.339788Z"
    },
    "papermill": {
     "duration": 2.324179,
     "end_time": "2025-12-12T13:08:58.342585",
     "exception": false,
     "start_time": "2025-12-12T13:08:56.018406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Writing all pipeline source files with updated reporter.py...\n",
      "  [+] Created: pipeline/factor/prime_hunter.py\n",
      "  [+] Created: pipeline/make_corpus.py\n",
      "  [+] Created: pipeline/extract_features.py\n",
      "  [+] Created: pipeline/triage.py\n",
      "  [+] Created: pipeline/predictive_triage.py\n",
      "  [+] Created: pipeline/orchestration.py\n",
      "  [+] Created: pipeline/report/reporter.py\n",
      "  ✅ All pipeline source files created.\n",
      "\n",
      "▶ Reloading pipeline modules to ensure changes are active...\n",
      "  ✅ Reloaded pipeline.make_corpus\n",
      "  ✅ Reloaded pipeline.factor.prime_hunter\n",
      "  ✅ Reloaded pipeline.extract_features\n",
      "  ✅ Reloaded pipeline.triage\n",
      "  ✅ Reloaded pipeline.predictive_triage\n",
      "  ✅ Reloaded pipeline.orchestration\n",
      "  ✅ Reloaded pipeline.report.reporter\n",
      "  ✅ All relevant modules reloaded.\n",
      "\n",
      "--- Re-running Corpus Generation ---\n",
      "Corpus saved to ./Prometheus_AIMO/pipeline/data/corpus.json\n",
      "\n",
      "--- Re-running Feature Extraction ---\n",
      "Features saved to ./Prometheus_AIMO/pipeline/data/features.csv\n",
      "\n",
      "--- Re-running Triage ---\n",
      "Triage results saved to ./Prometheus_AIMO/pipeline/data/triage_results.csv\n",
      "\n",
      "--- Re-running Predictive Triage Stage (with Cross-Validation) ---\n",
      "Loaded features from ./Prometheus_AIMO/pipeline/data/features.csv. Shape: (50, 8)\n",
      "Loaded triage results from ./Prometheus_AIMO/pipeline/data/triage_results.csv. Shape: (50, 7)\n",
      "Running predictive triage with ML model using 5-Fold Cross-Validation...\n",
      "  Processing Fold 1/5...\n",
      "  Processing Fold 2/5...\n",
      "  Processing Fold 3/5...\n",
      "  Processing Fold 4/5...\n",
      "  Processing Fold 5/5...\n",
      "\n",
      "Overall Cross-Validation Classification Report:\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "Factorable (Trial Division Optimized)       1.00      1.00      1.00        10\n",
      "                                 Hard       1.00      1.00      1.00         6\n",
      "                               Medium       1.00      1.00      1.00        34\n",
      "\n",
      "                             accuracy                           1.00        50\n",
      "                            macro avg       1.00      1.00      1.00        50\n",
      "                         weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "Predictive triage applied successfully with Cross-Validation.\n",
      "Predictive triage results saved to ./Prometheus_AIMO/pipeline/data/predicted_features.csv\n",
      "\n",
      "✅ Successfully generated predictive triage results: ./Prometheus_AIMO/pipeline/data/predicted_features.csv\n",
      "File size: 3981 bytes\n",
      "\n",
      "Cross-Validation Report:\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "Factorable (Trial Division Optimized)       1.00      1.00      1.00        10\n",
      "                                 Hard       1.00      1.00      1.00         6\n",
      "                               Medium       1.00      1.00      1.00        34\n",
      "\n",
      "                             accuracy                           1.00        50\n",
      "                            macro avg       1.00      1.00      1.00        50\n",
      "                         weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "\n",
      "--- Re-running Factoring Orchestration ---\n",
      "Loaded predicted features for orchestration from ./Prometheus_AIMO/pipeline/data/predicted_features.csv. Shape: (50, 10)\n",
      "Orchestrating factoring simulations...\n",
      "Factoring orchestration simulation complete.\n",
      "Saved factoring results to ./Prometheus_AIMO/pipeline/data/factoring_results.csv. Shape: (50, 12)\n",
      "\n",
      "--- Executing Report Generation ---\n",
      "DEBUG: Reporter loading factoring results from: ./Prometheus_AIMO/pipeline/data/factoring_results.csv\n",
      "DEBUG: Merged DataFrame columns: ['id', 'n', 'bit_length', 'num_digits', 'is_crypto_asset', 'factors', 'normalized_delta', 'helix_value', 'predicted_triage_category', 'predicted_triage_score', 'factoring_time', 'factoring_success_status']\n",
      "Factoring summary report generated at ./Prometheus_AIMO/pipeline/data/outputs/factoring_summary_report_20251212_130858.md\n",
      "✅ Factoring summary report successfully generated: ./Prometheus_AIMO/pipeline/data/outputs/factoring_summary_report_20251212_130858.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "\n",
    "# Re-define ROOT_DIR and create_file for self-containment in this step\n",
    "ROOT_DIR = \"./Prometheus_AIMO\"\n",
    "def create_file(rel_path, content):\n",
    "    path = os.path.join(ROOT_DIR, rel_path)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding='utf-8') as f:\n",
    "        f.write(content.strip())\n",
    "    print(f\"  [+] Created: {rel_path}\")\n",
    "\n",
    "# --- 1. Define Python code content for pipeline/factor/prime_hunter.py ---\n",
    "FILE_PRIME_HUNTER = r'''\n",
    "import math\n",
    "import random\n",
    "\n",
    "def is_prime(n):\n",
    "    \"\"\"\n",
    "    Checks if an integer n is prime using an optimized trial division method.\n",
    "    \"\"\"\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2 or n == 3:\n",
    "        return True\n",
    "    if n % 2 == 0 or n % 3 == 0:\n",
    "        return False\n",
    "\n",
    "    i = 5\n",
    "    while i * i <= n:\n",
    "        if n % i == 0 or n % (i + 2) == 0:\n",
    "            return False\n",
    "        i += 6\n",
    "    return True\n",
    "\n",
    "def trial_division_factorize(n):\n",
    "    \"\"\"\n",
    "    Factorizes an integer n into its prime factors using trial division.\n",
    "    \"\"\"\n",
    "    factors = []\n",
    "\n",
    "    # Handle factors of 2\n",
    "    while n % 2 == 0:\n",
    "        factors.append(2)\n",
    "        n //= 2\n",
    "\n",
    "    # Handle factors of 3\n",
    "    while n % 3 == 0:\n",
    "        factors.append(3)\n",
    "        n //= 3\n",
    "\n",
    "    # Handle factors from 5 onwards (i, i+2, i+4, ...) optimized as (i, i+2) with step 6\n",
    "    i = 5\n",
    "    while i * i <= n:\n",
    "        while n % i == 0:\n",
    "            factors.append(i)\n",
    "            n //= i\n",
    "        while n % (i + 2) == 0:\n",
    "            factors.append(i + 2)\n",
    "            n //= (i + 2)\n",
    "        i += 6\n",
    "\n",
    "    # If n is still greater than 1, it must be a prime factor itself\n",
    "    if n > 1:\n",
    "        factors.append(n)\n",
    "\n",
    "    return factors\n",
    "\n",
    "def pollard_rho_factorize(n, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    Finds a non-trivial factor of n using Pollard's Rho algorithm.\n",
    "    Returns a factor if found, otherwise None.\n",
    "    \"\"\"\n",
    "    if n % 2 == 0:\n",
    "        return 2\n",
    "    if n <= 1 or is_prime(n):\n",
    "        return None # n is prime or too small/invalid for this algorithm to find a *non-trivial* factor\n",
    "\n",
    "    x = random.randint(2, n - 1)\n",
    "    y = x\n",
    "    c = random.randint(1, n - 1)\n",
    "    d = 1 # gcd(x - y, n)\n",
    "\n",
    "    # Function for sequence generation (x^2 + c) mod n\n",
    "    def f(val):\n",
    "        return (val * val + c) % n\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        x = f(x)\n",
    "        y = f(f(y))\n",
    "        d = math.gcd(abs(x - y), n)\n",
    "\n",
    "        if d == n:\n",
    "            # Cycle detected, but factor is n itself. Try again with a different c or starting x.\n",
    "            # For simplicity, we'll return None for now, as re-starting is complex within this function.\n",
    "            return None\n",
    "        elif d != 1:\n",
    "            return d\n",
    "\n",
    "    return None # No factor found within max_iterations\n",
    "\n",
    "def is_semiprime(n):\n",
    "    \"\"\"\n",
    "    Checks if an integer n is a semiprime (a product of exactly two prime numbers).\n",
    "    \"\"\"\n",
    "    if n < 4:\n",
    "        return False\n",
    "    if is_prime(n):\n",
    "        return False\n",
    "\n",
    "    factors = trial_division_factorize(n)\n",
    "    return len(factors) == 2\n",
    "\n",
    "def prime_hunter_decompose(n):\n",
    "    \"\"\"\n",
    "    Orchestrates the factorization process for a given integer n.\n",
    "    Returns a dictionary containing prime factors, method used, and crypto asset status.\n",
    "    \"\"\"\n",
    "    # Ensure original_number is always included and handle non-positive/non-integer inputs correctly.\n",
    "    if not isinstance(n, int):\n",
    "        return {\"factors\": [], \"method\": \"Invalid Input (Not Integer)\", \"is_crypto_asset\": False, \"original_number\": n}\n",
    "    if n < 2:\n",
    "        return {\"factors\": [], \"method\": \"Invalid Input for Factorization\", \"is_crypto_asset\": False, \"original_number\": n}\n",
    "\n",
    "    initial_n = n\n",
    "    factors = []\n",
    "    method_used = \"\"\n",
    "    is_crypto_asset = False\n",
    "\n",
    "    # 1. Check if n is prime\n",
    "    if is_prime(n):\n",
    "        method_used = \"Prime Check\"\n",
    "        factors.append(n)\n",
    "    else:\n",
    "        # 2. Attempt trial division\n",
    "        td_factors = trial_division_factorize(n)\n",
    "        if td_factors and math.prod(td_factors) == n:\n",
    "            factors.extend(td_factors)\n",
    "            method_used = \"Trial Division\"\n",
    "        else:\n",
    "            # Trial division didn't fully factorize, or left a composite\n",
    "            remaining_n = n\n",
    "            temp_factors = []\n",
    "            if td_factors:\n",
    "                for f in td_factors:\n",
    "                    temp_factors.append(f)\n",
    "                    remaining_n //= f\n",
    "\n",
    "            # If there's still a composite remainder, try Pollard's Rho\n",
    "            if remaining_n > 1:\n",
    "                method_used = \"Pollard's Rho + Recursive Decomposition\"\n",
    "                queue = [remaining_n]\n",
    "                while queue:\n",
    "                    current_num = queue.pop(0)\n",
    "                    if is_prime(current_num):\n",
    "                        temp_factors.append(current_num)\n",
    "                        continue\n",
    "\n",
    "                    factor = pollard_rho_factorize(current_num)\n",
    "                    if factor and factor != 1 and factor != current_num:\n",
    "                        queue.append(factor)\n",
    "                        queue.append(current_num // factor)\n",
    "                    else:\n",
    "                        # Fallback for Pollard's Rho failure or non-trivial factor\n",
    "                        if is_prime(current_num):\n",
    "                            temp_factors.append(current_num)\n",
    "                        else:\n",
    "                            td_recheck_factors = trial_division_factorize(current_num)\n",
    "                            temp_factors.extend(td_recheck_factors)\n",
    "            else:\n",
    "                method_used = \"Trial Division\"\n",
    "            factors.extend(temp_factors)\n",
    "\n",
    "    # Ensure factors are sorted\n",
    "    factors.sort()\n",
    "\n",
    "    # Check for semiprime status of initial_n\n",
    "    if initial_n >= 4 and is_semiprime(initial_n):\n",
    "        is_crypto_asset = True\n",
    "\n",
    "    return {\n",
    "        \"factors\": factors,\n",
    "        \"method\": method_used,\n",
    "        \"is_crypto_asset\": is_crypto_asset,\n",
    "        \"original_number\": initial_n\n",
    "    }\n",
    "'''\n",
    "\n",
    "# --- 2. Define Python code content for pipeline/make_corpus.py ---\n",
    "FILE_MAKE_CORPUS = r'''\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_rsa_candidates(num_candidates, min_val=10, max_val=10000000):\n",
    "    \"\"\"\n",
    "    Generates synthetic RSA-like key candidates (semiprimes for now).\n",
    "    Updated min_val and max_val to create a more diverse range of numbers.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for i in range(num_candidates):\n",
    "        candidate = random.randint(min_val, max_val)\n",
    "        candidates.append({\"id\": i + 1, \"n\": candidate, \"original_problem\": f\"Analyze the RSA key candidate: {candidate}\"})\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def save_corpus(corpus_data, filepath=\"pipeline/data/corpus.json\"):\n",
    "    \"\"\"\n",
    "    Saves the generated corpus data to a JSON file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(corpus_data, f, indent=4)\n",
    "    print(f\"Corpus saved to {filepath}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    corpus = generate_rsa_candidates(num_candidates=50, min_val=10, max_val=10000000)\n",
    "    save_corpus(corpus, filepath=\"./Prometheus_AIMO/pipeline/data/corpus.json\")\n",
    "'''\n",
    "\n",
    "# --- 3. Define Python code content for pipeline/extract_features.py ---\n",
    "FILE_EXTRACT_FEATURES = r'''\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from pipeline.factor.prime_hunter import prime_hunter_decompose # Import the prime_hunter_decompose function\n",
    "\n",
    "def load_corpus(filepath=\"pipeline/data/corpus.json\"):\n",
    "    \"\"\"\n",
    "    Loads corpus data from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding='utf-8') as f:\n",
    "        corpus_data = json.load(f)\n",
    "    print(f\"Corpus loaded from {filepath}\")\n",
    "    return corpus_data\n",
    "\n",
    "def extract_numerical_features(candidate_data):\n",
    "    \"\"\"\n",
    "    Extracts numerical features from a candidate (e.g., bit length, number of digits),\n",
    "    integrates prime factorization information, and calculates predictive triage features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for item in candidate_data:\n",
    "        n = item[\"n\"]\n",
    "        if isinstance(n, int) and n > 0:\n",
    "            bit_length = n.bit_length()\n",
    "            num_digits = len(str(n))\n",
    "\n",
    "            # Integrate prime_hunter_decompose\n",
    "            decomposition_result = prime_hunter_decompose(n)\n",
    "            is_crypto_asset = decomposition_result.get(\"is_crypto_asset\", False)\n",
    "            factors_str = str(decomposition_result.get(\"factors\", [])) # Convert list to string for CSV\n",
    "\n",
    "            # Calculate Normalized Delta (bit_length to num_digits ratio)\n",
    "            normalized_delta = bit_length / num_digits if num_digits > 0 else 0.0\n",
    "\n",
    "            # Calculate Helix Value (a heuristic composite score)\n",
    "            # Higher for larger non-semiprimes, indicating potential difficulty\n",
    "            helix_value = (bit_length * (1 - int(is_crypto_asset))) # Convert bool to int for calculation\n",
    "\n",
    "            features.append({\n",
    "                \"id\": item[\"id\"],\n",
    "                \"n\": n,\n",
    "                \"bit_length\": bit_length,\n",
    "                \"num_digits\": num_digits,\n",
    "                \"is_crypto_asset\": is_crypto_asset,\n",
    "                \"factors\": factors_str,\n",
    "                \"normalized_delta\": normalized_delta,\n",
    "                \"helix_value\": helix_value\n",
    "            })\n",
    "    return features\n",
    "\n",
    "def save_features(features_data, filepath=\"pipeline/data/features.csv\"):\n",
    "    \"\"\"\n",
    "    Saves extracted features to a CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "        df = pd.DataFrame(features_data)\n",
    "        df.to_csv(filepath, index=False)\n",
    "    print(f\"Features saved to {filepath}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    corpus = load_corpus(filepath=\"./Prometheus_AIMO/pipeline/data/corpus.json\")\n",
    "    features = extract_numerical_features(corpus)\n",
    "    save_features(features, filepath=\"./Prometheus_AIMO/pipeline/data/features.csv\")\n",
    "'''\n",
    "\n",
    "# --- 4. Define Python code content for pipeline/triage.py ---\n",
    "FILE_TRIAGE = r'''\n",
    "import pandas as pd\n",
    "import os\n",
    "import math # Added import math\n",
    "import re # Added import re to parse factors string\n",
    "\n",
    "def load_features(filepath=\"pipeline/data/features.csv\"):\n",
    "    \"\"\"\n",
    "    Loads features from a CSV file.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def simple_triage(features_df):\n",
    "    \"\"\"\n",
    "    Implements a simple triage mechanism based on features (e.g., 'bond strength').\n",
    "    Updated thresholds to better classify a wider range of bit_lengths into Easy, Medium, and Hard.\n",
    "    Also introduces multi-dimensional triage for 'Crypto Asset' candidates.\n",
    "    \"\"\"\n",
    "    triage_results = []\n",
    "    for index, row in features_df.iterrows():\n",
    "        n = row[\"n\"]\n",
    "        bit_length = row[\"bit_length\"]\n",
    "        is_crypto_asset = row[\"is_crypto_asset\"]\n",
    "        factors_str = row[\"factors\"]\n",
    "\n",
    "        bond_strength = \"\"\n",
    "        triage_category = \"\"\n",
    "\n",
    "        if is_crypto_asset:\n",
    "            # Parse factors string (e.g., \"[p, q]\")\n",
    "            # Use regex to find numbers within brackets\n",
    "            parsed_factors = [int(f) for f in re.findall(r'\\d+', factors_str)]\n",
    "\n",
    "            if len(parsed_factors) == 2: # Ensure exactly two factors for semiprime\n",
    "                p, q = sorted(parsed_factors) # Ensure p <= q\n",
    "\n",
    "                # Multi-dimensional triage for Crypto Assets\n",
    "                if p == q: # Perfect square, e.g., 25 = 5*5\n",
    "                    bond_strength = \"Very High\"\n",
    "                    triage_category = \"Factorable (Square Root)\"\n",
    "                elif abs(p - q) < 5 * math.log(n): # Factors are close, amenable to Fermat's method\n",
    "                    bond_strength = \"High\"\n",
    "                    triage_category = \"Factorable (Fermat)\"\n",
    "                elif p < 1000: # One factor is small, amenable to Trial Division optimizations\n",
    "                    bond_strength = \"Medium\"\n",
    "                    triage_category = \"Factorable (Trial Division Optimized)\"\n",
    "                else: # Harder semiprime cases\n",
    "                    bond_strength = \"High\"\n",
    "                    triage_category = \"Hard (Semiprime)\"\n",
    "            else:\n",
    "                # Should ideally not happen if is_semiprime is accurate, but as a fallback\n",
    "                bond_strength = \"Medium\"\n",
    "                triage_category = \"Medium (Semiprime - factors unclear)\"\n",
    "        else: # Not a crypto asset (not a semiprime)\n",
    "            # Existing bit_length-based classification\n",
    "            if bit_length < 12: # e.g., numbers up to 2^12 - 1 = 4095\n",
    "                bond_strength = \"Low\"\n",
    "                triage_category = \"Easy\"\n",
    "            elif bit_length < 24: # e.g., numbers up to 2^24 - 1 = 16,777,215\n",
    "                bond_strength = \"Medium\"\n",
    "                triage_category = \"Medium\"\n",
    "            else: # Larger numbers\n",
    "                bond_strength = \"High\"\n",
    "                triage_category = \"Hard\"\n",
    "\n",
    "        triage_results.append({\n",
    "            \"id\": row[\"id\"],\n",
    "            \"n\": n,\n",
    "            \"bit_length\": bit_length,\n",
    "            \"bond_strength\": bond_strength,\n",
    "            \"triage_category\": triage_category,\n",
    "            \"is_crypto_asset\": is_crypto_asset,\n",
    "            \"factors\": factors_str\n",
    "        })\n",
    "    return pd.DataFrame(triage_results)\n",
    "\n",
    "def save_triage_results(triage_df, filepath=\"pipeline/data/triage_results.csv\"):\n",
    "    \"\"\"\n",
    "    Saves triage results to a CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "        triage_df.to_csv(filepath, index=False)\n",
    "    print(f\"Triage results saved to {filepath}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    features_df = load_features(filepath=\"./Prometheus_AIMO/pipeline/data/features.csv\")\n",
    "    triage_df = simple_triage(features_df)\n",
    "    save_triage_results(triage_df, filepath=\"./Prometheus_AIMO/pipeline/data/triage_results.csv\")\n",
    "'''\n",
    "\n",
    "# --- 5. Define Python code content for pipeline/predictive_triage.py ---\n",
    "FILE_PREDICTIVE_TRIAGE = r'''\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold # Changed to StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np # Added for array operations\n",
    "\n",
    "def predictive_triage(features_df, triage_results_df, n_splits=5):\n",
    "    \"\"\"\n",
    "    Implements a machine learning-based predictive triage system with K-Fold Cross-Validation.\n",
    "    Trains a RandomForestClassifier to predict 'triage_category' based on numerical features.\n",
    "    \"\"\"\n",
    "    print(f\"Running predictive triage with ML model using {n_splits}-Fold Cross-Validation...\")\n",
    "\n",
    "    # Merge features with actual triage results to get the target variable\n",
    "    merged_df = pd.merge(features_df, triage_results_df[['id', 'triage_category']], on='id', how='left')\n",
    "\n",
    "    # Define features and target\n",
    "    # Convert is_crypto_asset to int for the model\n",
    "    X = merged_df[['bit_length', 'num_digits', 'is_crypto_asset', 'normalized_delta', 'helix_value']].copy()\n",
    "    X['is_crypto_asset'] = X['is_crypto_asset'].astype(int)\n",
    "    y = merged_df['triage_category']\n",
    "\n",
    "    # Encode target labels to numerical values\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Initialize K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_predictions = np.zeros(len(y_encoded)) # To store predictions for each sample\n",
    "    all_probabilities = np.zeros((len(y_encoded), len(le.classes_))) # To store probabilities\n",
    "\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y_encoded)):\n",
    "        print(f\"  Processing Fold {fold+1}/{n_splits}...\")\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42) # Re-initialize for each fold\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        fold_predictions = model.predict(X_test)\n",
    "        fold_probabilities_raw = model.predict_proba(X_test) # Raw probabilities from model for this fold\n",
    "\n",
    "        # Map fold_probabilities_raw to the full set of classes in all_probabilities\n",
    "        fold_probabilities_mapped = np.zeros((len(y_test), len(le.classes_))) # Temporary array matching full shape\n",
    "        for i, class_label_encoded in enumerate(model.classes_):\n",
    "            # Find the column index in the full 'all_probabilities' array for this class\n",
    "            global_class_idx = np.where(le.classes_ == le.inverse_transform([class_label_encoded]))[0][0]\n",
    "            fold_probabilities_mapped[:, global_class_idx] = fold_probabilities_raw[:, i]\n",
    "\n",
    "        all_predictions[test_index] = fold_predictions\n",
    "        all_probabilities[test_index] = fold_probabilities_mapped\n",
    "\n",
    "        # Optional: Store fold-specific report\n",
    "        # FIX: Ensure classification_report uses all known labels and target names\n",
    "        fold_report = classification_report(y_test, fold_predictions, labels=le.transform(le.classes_), target_names=le.classes_, output_dict=True, zero_division=0)\n",
    "        fold_reports.append(fold_report)\n",
    "\n",
    "    # Calculate overall classification report from all out-of-fold predictions\n",
    "    # FIX: Ensure classification_report uses all known labels and target names\n",
    "    overall_report_str = classification_report(y_encoded, all_predictions, labels=le.transform(le.classes_), target_names=le.classes_, zero_division=0)\n",
    "    print(\"\\nOverall Cross-Validation Classification Report:\")\n",
    "    print(overall_report_str)\n",
    "\n",
    "    # Add predictions to the features_df\n",
    "    features_df['predicted_triage_category'] = le.inverse_transform(all_predictions.astype(int))\n",
    "\n",
    "    # Assign a predictive score based on probability of the most difficult category\n",
    "    # Identify the 'Hardest' category in the sorted classes by the LabelEncoder\n",
    "    # This assumes a consistent ordering or a specific definition of 'hardest'.\n",
    "    # For this example, let's look for 'Hard', 'Hard (Semiprime)', or 'Predicted_Hard'.\n",
    "\n",
    "    hardest_class_indices = []\n",
    "    if 'Hard' in le.classes_: hardest_class_indices.append(np.where(le.classes_ == 'Hard')[0][0])\n",
    "    if 'Hard (Semiprime)' in le.classes_: hardest_class_indices.append(np.where(le.classes_ == 'Hard (Semiprime)')[0][0])\n",
    "    if 'Predicted_Hard' in le.classes_: hardest_class_indices.append(np.where(le.classes_ == 'Predicted_Hard')[0][0])\n",
    "\n",
    "    # FIX: Handle case where hardest_class_indices might be empty (e.g., if no hard categories in this dataset)\n",
    "    if hardest_class_indices and all_probabilities.shape[1] > 0:\n",
    "        # Ensure we only try to index columns that exist in all_probabilities\n",
    "        valid_hardest_indices = [idx for idx in hardest_class_indices if idx < all_probabilities.shape[1]]\n",
    "        if valid_hardest_indices:\n",
    "            features_df['predicted_triage_score'] = np.max(all_probabilities[:, valid_hardest_indices], axis=1) # Take max probability if multiple hardest classes\n",
    "        else:\n",
    "            features_df['predicted_triage_score'] = 0.0\n",
    "    else:\n",
    "        features_df['predicted_triage_score'] = 0.0 # Default if no 'Hard' class is found or no probabilities\n",
    "\n",
    "    print(\"Predictive triage applied successfully with Cross-Validation.\")\n",
    "    return features_df, overall_report_str\n",
    "\n",
    "def save_predictive_triage_results(predictive_df, filepath=\"pipeline/data/predictive_triage_results.csv\"):\n",
    "    \"\"\"\n",
    "    Saves predictive triage results to a CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"w\", encoding='utf-8') as f:\n",
    "        predictive_df.to_csv(filepath, index=False)\n",
    "    print(f\"Predictive triage results saved to {filepath}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage (assuming features.csv and triage_results.csv exist)\n",
    "    try:\n",
    "        features_filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\", \"features.csv\")\n",
    "        triage_results_filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\", \"triage_results.csv\")\n",
    "\n",
    "        features_df_loaded = pd.read_csv(features_filepath)\n",
    "        triage_results_df_loaded = pd.read_csv(triage_results_filepath)\n",
    "\n",
    "        predicted_df, cv_report = predictive_triage(features_df_loaded.copy(), triage_results_df_loaded, n_splits=5)\n",
    "        save_predictive_triage_results(predicted_df, filepath=\"./Prometheus_AIMO/pipeline/data/predicted_features.csv\")\n",
    "\n",
    "        print(\"\\nPredictive triage results head:\")\n",
    "        print(predicted_df.head())\n",
    "\n",
    "        # This will now include the overall_report_str directly from the function call\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Missing file for predictive triage: {e}\")\n",
    "        print(\"Please ensure extract_features.py and triage.py have been run first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "'''\n",
    "\n",
    "# --- 6. Define Python code content for pipeline/orchestration.py ---\n",
    "FILE_ORCHESTRATION = r'''\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import time # For simulating process time\n",
    "\n",
    "def orchestrate_factoring(predicted_features_df):\n",
    "    \"\"\"\n",
    "    Simulates the application of factoring algorithms based on predicted_triage_category.\n",
    "    Records simulated factoring_time and factoring_success_status.\n",
    "    \"\"\"\n",
    "    print(\"Orchestrating factoring simulations...\")\n",
    "    factoring_results = []\n",
    "\n",
    "    for index, row in predicted_features_df.iterrows():\n",
    "        category = row['predicted_triage_category']\n",
    "        factoring_time = 0.0\n",
    "        factoring_success_status = False\n",
    "\n",
    "        # Simulate factoring based on category\n",
    "        if category == 'Factorable (Square Root)':\n",
    "            factoring_time = random.uniform(0.01, 0.1) # Very fast\n",
    "            factoring_success_status = True\n",
    "        elif category == 'Factorable (Fermat)':\n",
    "            factoring_time = random.uniform(0.1, 0.5) # Fast\n",
    "            factoring_success_status = True\n",
    "        elif category == 'Factorable (Trial Division Optimized)':\n",
    "            factoring_time = random.uniform(0.05, 0.3) # Moderate speed for small factors\n",
    "            factoring_success_status = True\n",
    "        elif category == 'Medium' or category == 'Medium (Semiprime - factors unclear)':\n",
    "            factoring_time = random.uniform(0.5, 2.0) # Slower, more effort\n",
    "            factoring_success_status = random.choice([True, False, False]) # 1/3 chance of failure\n",
    "        elif category == 'Hard' or category == 'Hard (Semiprime)':\n",
    "            factoring_time = random.uniform(2.0, 10.0) # Much slower, high effort\n",
    "            factoring_success_status = random.choice([True, False, False, False, False]) # 1/5 chance of success\n",
    "        else:\n",
    "            # Default for any unhandled categories\n",
    "            factoring_time = random.uniform(0.1, 1.0)\n",
    "            factoring_success_status = random.choice([True, False])\n",
    "\n",
    "        factoring_results.append({\n",
    "            'id': row['id'],\n",
    "            'factoring_time': round(factoring_time, 3),\n",
    "            'factoring_success_status': factoring_success_status\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(factoring_results)\n",
    "    # Merge original features with new factoring results\n",
    "    final_df = pd.merge(predicted_features_df, results_df, on='id', how='left')\n",
    "    print(\"Factoring orchestration simulation complete.\")\n",
    "    return final_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define file paths\n",
    "    ROOT_DIR = \"./Prometheus_AIMO\"\n",
    "    predicted_features_filepath = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"predicted_features.csv\")\n",
    "    factoring_results_filepath = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"factoring_results.csv\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(factoring_results_filepath), exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Load predicted_features.csv\n",
    "        predicted_df = pd.read_csv(predicted_features_filepath)\n",
    "        print(f\"Loaded predicted features from {predicted_features_filepath}. Shape: {predicted_df.shape}\")\n",
    "\n",
    "        # Orchestrate factoring\n",
    "        factored_df = orchestrate_factoring(predicted_df)\n",
    "\n",
    "        # Save results to factoring_results.csv\n",
    "        factored_df.to_csv(factoring_results_filepath, index=False)\n",
    "        print(f\"Saved factoring results to {factoring_results_filepath}. Shape: {factored_df.shape}\")\n",
    "\n",
    "        print(\"\\n--- Factoring Results Head ---\")\n",
    "        print(factored_df.head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Missing file. Ensure {predicted_features_filepath} exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during orchestration: {e}\")\n",
    "'''\n",
    "\n",
    "# --- 7. Define Python code content for pipeline/report/reporter.py (UPDATED) ---\n",
    "FILE_REPORTER = r'''\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "# No matplotlib.pyplot for text-based graph\n",
    "\n",
    "def generate_factoring_summary_report(factoring_results_filepath=\"pipeline/data/factoring_results.csv\",\n",
    "                                      output_dir=\"pipeline/data/outputs\"):\n",
    "    \"\"\"\n",
    "    Generates a comprehensive summary report based on factoring orchestration results,\n",
    "    including efficiency metrics and a simulated 'Money Graph'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load data - factoring_results.csv already contains predicted_triage_category\n",
    "        print(f\"DEBUG: Reporter loading factoring results from: {factoring_results_filepath}\")\n",
    "        merged_df = pd.read_csv(factoring_results_filepath) # Direct load of the complete dataframe\n",
    "        print(f\"DEBUG: Merged DataFrame columns: {merged_df.columns.tolist()}\")\n",
    "\n",
    "        # Calculate efficiency metrics\n",
    "        total_candidates = len(merged_df)\n",
    "        successful_factorizations = merged_df['factoring_success_status'].sum()\n",
    "        success_percentage = (successful_factorizations / total_candidates) * 100 if total_candidates > 0 else 0\n",
    "\n",
    "        # Metrics per predicted_triage_category\n",
    "        category_metrics = merged_df.groupby('predicted_triage_category').agg(\n",
    "            total_count=('id', 'size'),\n",
    "            successful_count=('factoring_success_status', lambda x: x.sum()),\n",
    "            avg_time=('factoring_time', 'mean')\n",
    "        ).reset_index()\n",
    "        category_metrics['success_rate'] = (category_metrics['successful_count'] / category_metrics['total_count']) * 100\n",
    "\n",
    "        # Simulate 'Money Graph' - textual representation of ROI\n",
    "        # Assume 'investment' is factoring time, 'return' is successful factorization\n",
    "        total_investment = merged_df['factoring_time'].sum()\n",
    "        successful_investment = merged_df[merged_df['factoring_success_status']]['factoring_time'].sum()\n",
    "\n",
    "        money_graph_str = \"\"\"\n",
    "## Simulated 'Money Graph' (Efficiency Overview):\n",
    "\n",
    "Total Investment (Simulated Factoring Time): {total_investment:.2f} units\n",
    "Successful Investment (Time on successful factors): {successful_investment:.2f} units\n",
    "\n",
    "[Investment] {investment_bar}\n",
    "[Return   ] {return_bar}\n",
    "\"\"\"\n",
    "        bar_length = 50\n",
    "        # Correct calculation of investment_ratio (should always be 1 for total investment bar)\n",
    "        investment_ratio_display = 1.0 # The investment bar always represents 100% of total investment\n",
    "        return_ratio_display = successful_investment / (total_investment + 1e-9) if total_investment > 0 else 0\n",
    "\n",
    "        investment_bar = '#' * int(bar_length * investment_ratio_display) + '-' * (bar_length - int(bar_length * investment_ratio_display))\n",
    "        return_bar = '*' * int(bar_length * return_ratio_display) + ' ' * (bar_length - int(bar_length * return_ratio_display))\n",
    "\n",
    "        money_graph_content = money_graph_str.format(\n",
    "            total_investment=total_investment,\n",
    "            successful_investment=successful_investment,\n",
    "            investment_bar=investment_bar,\n",
    "            return_bar=return_bar\n",
    "        )\n",
    "\n",
    "        # Compile comprehensive report\n",
    "        report_content = f\"\"\"\n",
    "# Prometheus AIMO Factoring Efficiency Report\n",
    "\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Overall Factoring Metrics:\n",
    "- Total Candidates Processed: {total_candidates}\n",
    "- Successful Factorizations: {int(successful_factorizations)} ({success_percentage:.2f}%)\n",
    "- Total Simulated Factoring Time: {total_investment:.2f} units\n",
    "\n",
    "## Metrics per Predicted Triage Category:\n",
    "{category_metrics.to_markdown(index=False)}\n",
    "\n",
    "{money_graph_content}\n",
    "\n",
    "## Raw Data Head (Factoring results):\n",
    "{merged_df.head().to_markdown(index=False)}\n",
    "\"\"\"\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        report_filepath = os.path.join(output_dir, f\"factoring_summary_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\")\n",
    "        with open(report_filepath, \"w\", encoding='utf-8') as f:\n",
    "            f.write(report_content)\n",
    "        print(f\"Factoring summary report generated at {report_filepath}\")\n",
    "        return report_filepath\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Missing file for report generation: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during report generation: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage for generate_factoring_summary_report\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\")) # Adjust ROOT_DIR for standalone execution\n",
    "    factoring_results_filepath_main = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"factoring_results.csv\") # Only need factoring results\n",
    "    output_dir_main = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"outputs\")\n",
    "\n",
    "    if os.path.exists(factoring_results_filepath_main):\n",
    "        generate_factoring_summary_report(\n",
    "            factoring_results_filepath=factoring_results_filepath_main,\n",
    "            output_dir=output_dir_main\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Error: Required input file for report not found. Make sure all previous pipeline steps have run.\")\n",
    "\n",
    "    # The original generate_summary_report might still be used for simple triage reports if needed separately.\n",
    "    # generate_summary_report(\n",
    "    #    triage_results_filepath=\"./Prometheus_AIMO/pipeline/data/triage_results.csv\",\n",
    "    #    output_dir=\"./Prometheus_AIMO/pipeline/data/outputs\"\n",
    "    #)\n",
    "'''\n",
    "\n",
    "print(\"\\n▶ Writing all pipeline source files with updated reporter.py...\")\n",
    "create_file(\"pipeline/factor/prime_hunter.py\", FILE_PRIME_HUNTER)\n",
    "create_file(\"pipeline/make_corpus.py\", FILE_MAKE_CORPUS)\n",
    "create_file(\"pipeline/extract_features.py\", FILE_EXTRACT_FEATURES)\n",
    "create_file(\"pipeline/triage.py\", FILE_TRIAGE)\n",
    "create_file(\"pipeline/predictive_triage.py\", FILE_PREDICTIVE_TRIAGE)\n",
    "create_file(\"pipeline/orchestration.py\", FILE_ORCHESTRATION) # Include orchestration.py as it's now part of the pipeline\n",
    "create_file(\"pipeline/report/reporter.py\", FILE_REPORTER)\n",
    "print(\"  ✅ All pipeline source files created.\")\n",
    "\n",
    "# Adjust sys.path to allow importing from pipeline\n",
    "project_root = os.path.abspath(ROOT_DIR)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# --- Reload modules ---\n",
    "# Reload relevant modules to ensure changes are active\n",
    "print(\"\\n▶ Reloading pipeline modules to ensure changes are active...\")\n",
    "try:\n",
    "    import pipeline.make_corpus as make_corpus_module\n",
    "    importlib.reload(make_corpus_module)\n",
    "    print(\"  ✅ Reloaded pipeline.make_corpus\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Could not reload pipeline.make_corpus: {e}\")\n",
    "try:\n",
    "    import pipeline.factor.prime_hunter as prime_hunter_module\n",
    "    importlib.reload(prime_hunter_module)\n",
    "    print(\"  ✅ Reloaded pipeline.factor.prime_hunter\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Could not reload pipeline.factor.prime_hunter: {e}\")\n",
    "try:\n",
    "    import pipeline.extract_features as extract_features_module\n",
    "    importlib.reload(extract_features_module)\n",
    "    print(\"  ✅ Reloaded pipeline.extract_features\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Could not reload pipeline.extract_features: {e}\")\n",
    "try:\n",
    "    import pipeline.triage as triage_module\n",
    "    importlib.reload(triage_module)\n",
    "    print(\"  ✅ Reloaded pipeline.triage\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Could not reload pipeline.triage: {e}\")\n",
    "try:\n",
    "    import pipeline.predictive_triage as predictive_triage_module\n",
    "    importlib.reload(predictive_triage_module)\n",
    "    print(\"  ✅ Reloaded pipeline.predictive_triage\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Could not reload pipeline.predictive_triage: {e}\")\n",
    "try:\n",
    "    import pipeline.orchestration as orchestration_module\n",
    "    importlib.reload(orchestration_module)\n",
    "    print(\"  ✅ Reloaded pipeline.orchestration\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Could not reload pipeline.orchestration: {e}\")\n",
    "try:\n",
    "    import pipeline.report.reporter as reporter_module\n",
    "    importlib.reload(reporter_module)\n",
    "    print(\"  ✅ Reloaded pipeline.report.reporter\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Could not reload pipeline.report.reporter: {e}\")\n",
    "\n",
    "print(\"  ✅ All relevant modules reloaded.\")\n",
    "\n",
    "# --- Re-run Corpus Generation with updated range ---\n",
    "print(\"\\n--- Re-running Corpus Generation ---\")\n",
    "corpus_data = make_corpus_module.generate_rsa_candidates(num_candidates=50, min_val=10, max_val=10000000)\n",
    "corpus_filepath = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"corpus.json\")\n",
    "make_corpus_module.save_corpus(corpus_data, filepath=corpus_filepath)\n",
    "\n",
    "# --- Re-run Feature Extraction with updated logic ---\n",
    "print(\"\\n--- Re-running Feature Extraction ---\")\n",
    "features_data = extract_features_module.extract_numerical_features(corpus_data)\n",
    "features_filepath = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"features.csv\")\n",
    "extract_features_module.save_features(features_data, filepath=features_filepath)\n",
    "\n",
    "# --- Re-run Triage with Multi-Dimensional Categories ---\n",
    "print(\"\\n--- Re-running Triage ---\")\n",
    "features_df_triage = triage_module.load_features(filepath=features_filepath)\n",
    "triage_df = triage_module.simple_triage(features_df_triage)\n",
    "triage_results_filepath = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"triage_results.csv\")\n",
    "triage_module.save_triage_results(triage_df, filepath=triage_results_filepath)\n",
    "\n",
    "# --- Re-run Predictive Triage with ML Model and Cross-Validation ---\n",
    "print(\"\\n--- Re-running Predictive Triage Stage (with Cross-Validation) ---\")\n",
    "predicted_features_filepath = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"predicted_features.csv\")\n",
    "\n",
    "try:\n",
    "    features_df_ml = pd.read_csv(features_filepath)\n",
    "    triage_results_df_ml = pd.read_csv(triage_results_filepath)\n",
    "    print(f\"Loaded features from {features_filepath}. Shape: {features_df_ml.shape}\")\n",
    "    print(f\"Loaded triage results from {triage_results_filepath}. Shape: {triage_results_df_ml.shape}\")\n",
    "\n",
    "    predicted_features_df, cv_report = predictive_triage_module.predictive_triage(features_df_ml.copy(), triage_results_df_ml, n_splits=5)\n",
    "    predictive_triage_module.save_predictive_triage_results(predicted_features_df, filepath=predicted_features_filepath)\n",
    "\n",
    "    print(f\"\\n✅ Successfully generated predictive triage results: {predicted_features_filepath}\")\n",
    "    print(f\"File size: {os.path.getsize(predicted_features_filepath)} bytes\")\n",
    "\n",
    "    print(\"\\nCross-Validation Report:\")\n",
    "    print(cv_report)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Missing file. Ensure features.csv and triage_results.csv exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during predictive triage execution: {e}\")\n",
    "\n",
    "# --- Re-run Factoring Orchestration ---\n",
    "print(\"\\n--- Re-running Factoring Orchestration ---\")\n",
    "factoring_results_filepath = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"factoring_results.csv\")\n",
    "\n",
    "try:\n",
    "    predicted_df_orchestration = pd.read_csv(predicted_features_filepath)\n",
    "    print(f\"Loaded predicted features for orchestration from {predicted_features_filepath}. Shape: {predicted_df_orchestration.shape}\")\n",
    "    \n",
    "    factored_df_simulated = orchestration_module.orchestrate_factoring(predicted_df_orchestration.copy())\n",
    "    factored_df_simulated.to_csv(factoring_results_filepath, index=False)\n",
    "    print(f\"Saved factoring results to {factoring_results_filepath}. Shape: {factored_df_simulated.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Missing file for orchestration. Ensure {predicted_features_filepath} exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during orchestration execution: {e}\")\n",
    "\n",
    "# --- Execute Report Generation ---\n",
    "print(\"\\n--- Executing Report Generation ---\")\n",
    "\n",
    "try:\n",
    "    # Only factoring_results_filepath_report is needed now, as it contains all info\n",
    "    factoring_results_filepath_report = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"factoring_results.csv\")\n",
    "    output_dir_report = os.path.join(ROOT_DIR, \"pipeline\", \"data\", \"outputs\")\n",
    "\n",
    "    if os.path.exists(factoring_results_filepath_report):\n",
    "        report_file = reporter_module.generate_factoring_summary_report(\n",
    "            factoring_results_filepath=factoring_results_filepath_report,\n",
    "            output_dir=output_dir_report\n",
    "        )\n",
    "        if report_file:\n",
    "            print(f\"✅ Factoring summary report successfully generated: {report_file}\")\n",
    "    else:\n",
    "        print(f\"❌ Error: Required input file for report not found. Make sure all previous pipeline steps have run.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during report generation execution: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "isSourceIdPinned": false,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 495573,
     "modelInstanceId": 479860,
     "sourceId": 636847,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.803024,
   "end_time": "2025-12-12T13:08:58.762799",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-12T13:08:50.959775",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
